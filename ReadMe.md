# ğŸ“Œ PrÃ©diction du Risque de CrÃ©dit avec Azure ML et Optuna

Ce projet a pour objectif de **prÃ©dire le risque de dÃ©faut de crÃ©dit** Ã  partir de donnÃ©es financiÃ¨res et dÃ©mographiques, en utilisant **Azure ML**, **Optuna** pour l'optimisation des hyperparamÃ¨tres, et **MLflow** pour le suivi des expÃ©rimentations sur Azure Cloud.

---

## ğŸ¯ **1. Contexte du Projet**
### ğŸ“Œ **1.1. Objectif du Dataset**
- **Dataset utilisÃ©** : [Give Me Some Credit](https://www.kaggle.com/c/GiveMeSomeCredit).
- **Objectif** : PrÃ©dire si un client fera dÃ©faut dans les **2 prochaines annÃ©es** (la variable cible est : `SeriousDlqin2yrs`).
- **ProblÃ©matique** :
  - **DonnÃ©es dÃ©sÃ©quilibrÃ©es**
  - **PrÃ©sence de valeurs aberrantes**
  - **DonnÃ©es manquantes** (`MonthlyIncome`, `NumberOfDependents`).
  - **InterprÃ©tabilitÃ©**

---

## ğŸ”¥ **2. Approche MÃ©thodologique**
Ce projet sâ€™articule autour de **trois grandes Ã©tapes** :

### âœ… **2.1 Analyse Exploratoire et PrÃ©traitement**
- **Nettoyage des donnÃ©es** : Gestion des valeurs manquantes (remplissage par mÃ©diane, imputations).
- **Transformation des variables** : Log-transformation pour rÃ©duire lâ€™asymÃ©trie.
- **DÃ©tection des outliers** : MÃ©thodes basÃ©es sur la distribution (boxplots, kurtosis).

---

### ğŸ¤– **2.2 Optimisation des ModÃ¨les**
Nous avons testÃ© plusieurs modÃ¨les :
- **RÃ©gression Logistique** : ModÃ¨le classique et interprÃ©table en classification binaire. Il sert de baseline et permet de comprendre l'impact des variables sur la probabilitÃ© de dÃ©faut.
- **Random Forest** : Algorithme robuste et efficace, particuliÃ¨rement en prÃ©sence de bruit et de donnÃ©es hÃ©tÃ©rogÃ¨nes. GrÃ¢ce Ã  son approche ensembliste, il rÃ©duit le risque de sur-apprentissage et capture des interactions complexes entre les variables.
- **XGBoost** : Algorithmes de boosting puissants, particuliÃ¨rement adaptÃ©s aux jeux de donnÃ©es dÃ©sÃ©quilibrÃ©s. Leur capacitÃ© Ã  pondÃ©rer les erreurs et Ã  ajuster dynamiquement l'importance des classes permet d'amÃ©liorer la performance prÃ©dictive, notamment sur la minoritÃ© des cas de dÃ©tresse financiÃ¨re.
- **LightGBM** : Alternative performante Ã  XGBoost, LightGBM est optimisÃ© pour les grands volumes de donnÃ©es et offre une meilleure rapiditÃ© d'entraÃ®nement. Suite Ã  l'utilisation `d'AutoML` sur Azure, ce modÃ¨le a obtenu d'excellentes performances, ce qui m'a motivÃ© son intÃ©gration dans notre comparaison.

#### ğŸ” **Pourquoi utiliser Optuna pour l'optimisation des hyperparamÃ¨tres ?**
- **Optimisation bayÃ©sienne** `L'estimation de densitÃ© de Parzen` avec Optuna est plus flexibles que les modÃ¨les gaussiens classiques.
- **Exploration efficace du paramÃ¨tre espace**.

Les **hyperparamÃ¨tres optimisÃ©s** :
- **XGBoost** : `n_estimators`, `learning_rate`, `max_depth`
- **LightGBM** : `n_estimators`, `learning_rate`
- **RandomForest** : `n_estimators`, `max_depth`
- **Logistic Regression** : `C`

---

### ğŸ“ˆ **2.3 Ã‰valuation des RÃ©sultats**
Nous avons suivi **trois mÃ©triques clÃ©s** :
1. **AUC-ROC** : Mesure la capacitÃ© Ã  discriminer entre bons et mauvais clients.
2. **F1-score** : Prend en compte le dÃ©sÃ©quilibre des classes.
3. **Gini** : 

Les **modÃ¨les de boosting** (XGBoost, LightGBM) offrent **les meilleurs scores AUC et donc Gini**.

---

## ğŸ“Š **3. Analyse des RÃ©sultats**
### ğŸ“Œ **3.1 Importance des Variables (SHAP & AzureML Interpret)**
- **`RevolvingUtilizationOfUnsecuredLines`** : Indicateur clÃ© du dÃ©faut.
- **`DebtRatio`** : Plus Ã©levÃ© chez les clients en dÃ©faut.
- **`MonthlyIncome`** : Impacte directement la solvabilitÃ©.

### ğŸ“Œ **3.2 Courbes de PrÃ©cision-Rappel**
- **Optimisation du seuil de dÃ©cision** pour Ã©quilibrer **prÃ©cision vs rappel**.

---

## ğŸ† **4. Soumission sur Kaggle**
Meilleur rÃ©sultat obtenue :
- **private** : 0.86472
- **public** : 0.85805


