[I 2025-02-04 14:37:43,533] A new study created in memory with name: no-name-17b7271f-cf46-488c-a79f-c57f4f437d8f
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:44,152] Trial 0 finished with value: 0.8600284299339691 and parameters: {'n_estimators': 50, 'learning_rate': 0.023019355441745142}. Best is trial 0 with value: 0.8600284299339691.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:45,747] Trial 1 finished with value: 0.8500780242969278 and parameters: {'n_estimators': 300, 'learning_rate': 0.18097771849129013}. Best is trial 0 with value: 0.8600284299339691.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:47,254] Trial 2 finished with value: 0.8657966259024008 and parameters: {'n_estimators': 200, 'learning_rate': 0.03450471962455487}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:48,139] Trial 3 finished with value: 0.8522472333889332 and parameters: {'n_estimators': 150, 'learning_rate': 0.21934326278466273}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:49,884] Trial 4 finished with value: 0.8596248853315018 and parameters: {'n_estimators': 300, 'learning_rate': 0.11149128007024557}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:51,171] Trial 5 finished with value: 0.8620614009114704 and parameters: {'n_estimators': 150, 'learning_rate': 0.012685762815947408}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:52,567] Trial 6 finished with value: 0.8652814608114312 and parameters: {'n_estimators': 200, 'learning_rate': 0.061079848954913384}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:53,054] Trial 7 finished with value: 0.8631811684051776 and parameters: {'n_estimators': 50, 'learning_rate': 0.05095658560806502}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:53,671] Trial 8 finished with value: 0.8521416020414729 and parameters: {'n_estimators': 100, 'learning_rate': 0.28385622171518043}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:55,987] Trial 9 finished with value: 0.8656123756982079 and parameters: {'n_estimators': 300, 'learning_rate': 0.018691296223369187}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:57,477] Trial 10 finished with value: 0.8657615817083193 and parameters: {'n_estimators': 200, 'learning_rate': 0.03940683619805082}. Best is trial 2 with value: 0.8657966259024008.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:37:58,997] Trial 11 finished with value: 0.8661671411639198 and parameters: {'n_estimators': 200, 'learning_rate': 0.03547658447125375}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:00,859] Trial 12 finished with value: 0.8660696721487984 and parameters: {'n_estimators': 250, 'learning_rate': 0.027917771011407654}. Best is trial 11 with value: 0.8661671411639198.
Index(['SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines', 'age',
       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',
       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',
       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',
       'NumberOfDependents', 'age_cat'],
      dtype='object')
Index(['SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines', 'age',
       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',
       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',
       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',
       'NumberOfDependents'],
      dtype='object')
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009903 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005889 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005064 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005291 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008631 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008305 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008272 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005694 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004407 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004684 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005782 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004508 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004521 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005732 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:02,847] Trial 13 finished with value: 0.8634868481471141 and parameters: {'n_estimators': 250, 'learning_rate': 0.010383198738642128}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:04,304] Trial 14 finished with value: 0.8619116684269649 and parameters: {'n_estimators': 250, 'learning_rate': 0.09440858964719728}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:05,658] Trial 15 finished with value: 0.8264104035267101 and parameters: {'n_estimators': 250, 'learning_rate': 0.48782673987430053}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:07,569] Trial 16 finished with value: 0.8658058015989875 and parameters: {'n_estimators': 250, 'learning_rate': 0.025867604612164696}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:08,802] Trial 17 finished with value: 0.8635685387642225 and parameters: {'n_estimators': 150, 'learning_rate': 0.017006531586560104}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:09,777] Trial 18 finished with value: 0.8643903573768358 and parameters: {'n_estimators': 100, 'learning_rate': 0.034865188771534134}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:11,132] Trial 19 finished with value: 0.8628121177716729 and parameters: {'n_estimators': 200, 'learning_rate': 0.09297335583412157}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:12,837] Trial 20 finished with value: 0.8653224506035598 and parameters: {'n_estimators': 250, 'learning_rate': 0.05409129311885378}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:14,705] Trial 21 finished with value: 0.8661068697419063 and parameters: {'n_estimators': 250, 'learning_rate': 0.026034392708481662}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:16,531] Trial 22 finished with value: 0.8659930998128316 and parameters: {'n_estimators': 250, 'learning_rate': 0.030256499135907427}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:18,130] Trial 23 finished with value: 0.8644656788413091 and parameters: {'n_estimators': 200, 'learning_rate': 0.0168227147876347}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:20,017] Trial 24 finished with value: 0.865237795104336 and parameters: {'n_estimators': 300, 'learning_rate': 0.044845729808219714}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:22,094] Trial 25 finished with value: 0.8656344203290495 and parameters: {'n_estimators': 250, 'learning_rate': 0.019840040319655305}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:23,656] Trial 26 finished with value: 0.8655714254906204 and parameters: {'n_estimators': 200, 'learning_rate': 0.026869261738535082}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:25,974] Trial 27 finished with value: 0.8651218678177808 and parameters: {'n_estimators': 300, 'learning_rate': 0.013715180109590962}. Best is trial 11 with value: 0.8661671411639198.
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004417 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004346 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004344 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004397 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009218 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008937 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008397 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004845 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004486 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004537 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004348 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007882 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004395 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004440 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004412 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:27,007] Trial 28 finished with value: 0.8649895802404612 and parameters: {'n_estimators': 150, 'learning_rate': 0.07478591000201586}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:28,836] Trial 29 finished with value: 0.8658898118701942 and parameters: {'n_estimators': 250, 'learning_rate': 0.02464861719211473}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:29,684] Trial 30 finished with value: 0.8651170147530634 and parameters: {'n_estimators': 100, 'learning_rate': 0.040979252881829686}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:31,448] Trial 31 finished with value: 0.8660746518840468 and parameters: {'n_estimators': 250, 'learning_rate': 0.03012324537440592}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:33,282] Trial 32 finished with value: 0.8657880439739283 and parameters: {'n_estimators': 250, 'learning_rate': 0.02307250142334041}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:34,746] Trial 33 finished with value: 0.865999144372231 and parameters: {'n_estimators': 200, 'learning_rate': 0.032444392352296625}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:36,418] Trial 34 finished with value: 0.8625889520052958 and parameters: {'n_estimators': 300, 'learning_rate': 0.07167247707440413}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:38,320] Trial 35 finished with value: 0.8650608363725816 and parameters: {'n_estimators': 200, 'learning_rate': 0.020936561145021064}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:40,672] Trial 36 finished with value: 0.8585872201341289 and parameters: {'n_estimators': 250, 'learning_rate': 0.13922838757344236}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:43,510] Trial 37 finished with value: 0.865417691009027 and parameters: {'n_estimators': 300, 'learning_rate': 0.014927148420038026}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:44,948] Trial 38 finished with value: 0.8656724016960677 and parameters: {'n_estimators': 150, 'learning_rate': 0.049990230527842724}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:47,559] Trial 39 finished with value: 0.8624816106056676 and parameters: {'n_estimators': 200, 'learning_rate': 0.010631174652891297}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:52,945] Trial 40 finished with value: 0.8658708033736416 and parameters: {'n_estimators': 300, 'learning_rate': 0.03104221556433902}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:38:58,219] Trial 41 finished with value: 0.8656026022750534 and parameters: {'n_estimators': 200, 'learning_rate': 0.034676840102686714}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:04,543] Trial 42 finished with value: 0.8654759000764582 and parameters: {'n_estimators': 200, 'learning_rate': 0.027077328999938283}. Best is trial 11 with value: 0.8661671411639198.
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004287 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004342 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004489 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004343 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004381 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004333 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004782 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040722 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005839 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006914 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005872 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015835 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006743 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051467 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018471 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:09,908] Trial 43 finished with value: 0.8651506774466666 and parameters: {'n_estimators': 150, 'learning_rate': 0.06098171294998981}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:16,539] Trial 44 finished with value: 0.8654210438196435 and parameters: {'n_estimators': 250, 'learning_rate': 0.03852385119701782}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:24,834] Trial 45 finished with value: 0.8658890874730952 and parameters: {'n_estimators': 250, 'learning_rate': 0.029903143207751805}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:31,152] Trial 46 finished with value: 0.8651395858582993 and parameters: {'n_estimators': 200, 'learning_rate': 0.023245741443476537}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:36,404] Trial 47 finished with value: 0.8652791847003279 and parameters: {'n_estimators': 250, 'learning_rate': 0.047822114672743284}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:42,871] Trial 48 finished with value: 0.8646453253633923 and parameters: {'n_estimators': 200, 'learning_rate': 0.018014549575645262}. Best is trial 11 with value: 0.8661671411639198.
train_model.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  "learning_rate": trial.suggest_loguniform("learning_rate", 0.01, 0.5),
[I 2025-02-04 14:39:47,684] Trial 49 finished with value: 0.8647135928626728 and parameters: {'n_estimators': 250, 'learning_rate': 0.05969404744945591}. Best is trial 11 with value: 0.8661671411639198.
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015397 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010773 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012416 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017213 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053423 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060376 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306

======================= Résultat =======================
✅ Meilleurs paramètres : {'n_estimators': 200, 'learning_rate': 0.03547658447125375}
✅ Meilleur AUC : 0.8661671411639198
✅ Meilleur F1-score : 0.27898643460455597
========================================================
[LightGBM] [Info] Number of positive: 7018, number of negative: 97982
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007337 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 969
[LightGBM] [Info] Number of data points in the train set: 105000, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066838 -> initscore=-2.636306
[LightGBM] [Info] Start training from score -2.636306
